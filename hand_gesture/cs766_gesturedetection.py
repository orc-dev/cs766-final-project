# -*- coding: utf-8 -*-
"""CS766_gestureDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdDk0dNyL8Z_DUcrjQUntJjSkNGPrkkd
"""

!wget -q -O rps.zip https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip
!unzip -qq rps.zip

"""Download of official tutorial dataset above

extract light weight Hagrid dataset from google drive
"""

import zipfile
from google.colab import drive

drive.mount('/content/drive/')

!unzip -u "/content/drive/My Drive/cs766_Hagrid.zip" -d "./new_data"

"""Intall suitable version of media pipe maker"""

!pip install 'keras<3.0.0' mediapipe-model-maker

"""crop Hagrid dataset using its own label"""

import json
import os

k = 0
class_need = ['palm','fist','one','like']
os.mkdir("./crop_data/")
os.mkdir("./crop_data/none")
none_count = 0
for classes in class_need:
  f = open('new_data/hagrid-sample-30k-384p/ann_train_val/'+classes+'.json')
  d = json.load(f)
  dir_name = './new_data/hagrid-sample-30k-384p/hagrid_30k/train_val_'+classes+'/'
  imgList = os.listdir(dir_name);
  os.mkdir("./crop_data/"+classes+"/")
  class_count = 0;

  for imgName in imgList:
    img = Image.open(dir_name+imgName)
    imgID=imgName[0:(len(imgName)-4)]
    # print(imgID)
    bboxes = d[imgID]['bboxes']
    labels = d[imgID]['labels']
    i = -1
    class_count += 1;
    if(class_count==200): continue
    for bbox in bboxes:
      i+=1;

      min_x = bbox[0]
      min_y = bbox[1]
      max_x = min_x+bbox[2]
      max_y = min_y+bbox[3]
      img = np.array(img)
      min_x*=img.shape[1]
      min_y*=img.shape[0]
      max_x*=img.shape[1]
      max_y*=img.shape[0]
      ans = Image.fromarray(img)

      box = (min_x, min_y, max_x, max_y)
      #print(box)
      ans = ans.crop(box)
      #cv2_imshow(cv2.cvtColor(np.array(ans), cv2.COLOR_BGR2RGB))
      if(labels[i]=="no_gesture"):
        if(none_count==200):continue
        cv2.imwrite("./crop_data/none/"+imgID+str(i)+".jpg", cv2.cvtColor(np.array(ans), cv2.COLOR_BGR2RGB))
        none_count+=1;
        continue;
      else:
        cv2.imwrite("./crop_data/"+classes+"/"+imgID+str(i)+".jpg", cv2.cvtColor(np.array(ans), cv2.COLOR_BGR2RGB))


  f.close()

"""remove the dataset directory for testing purpose"""

import shutil
shutil.rmtree("./crop_data/")

"""combining two datasets"""

src="./rps_data_sample/paper/"
dst="./crop_data/palm/"
file_list = os.listdir(src);
for file_name in file_list:
  shutil.move(src+file_name,dst+file_name)

src="./rps_data_sample/rock/"
dst="./crop_data/fist/"
file_list = os.listdir(src);
for file_name in file_list:
  shutil.move(src+file_name,dst+file_name)

"""get official hand detection model for croping of frame"""

!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task

!wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg

import cv2
from google.colab.patches import cv2_imshow

"""Turning result into human understandable version. Source: official tutorial:https://github.com/googlesamples/mediapipe/tree/main"""

from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np

MARGIN = 10  # pixels
FONT_SIZE = 1
FONT_THICKNESS = 1
HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green

def draw_landmarks_on_image(rgb_image, detection_result):
  hand_landmarks_list = detection_result.hand_landmarks
  handedness_list = detection_result.handedness
  annotated_image = np.copy(rgb_image)

  # Loop through the detected hands to visualize.
  for idx in range(len(hand_landmarks_list)):
    hand_landmarks = hand_landmarks_list[idx]
    handedness = handedness_list[idx]

    # Draw the hand landmarks.
    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    hand_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks
    ])
    solutions.drawing_utils.draw_landmarks(
      annotated_image,
      hand_landmarks_proto,
      solutions.hands.HAND_CONNECTIONS,
      solutions.drawing_styles.get_default_hand_landmarks_style(),
      solutions.drawing_styles.get_default_hand_connections_style())

    # Get the top left corner of the detected hand's bounding box.
    height, width, _ = annotated_image.shape
    x_coordinates = [landmark.x for landmark in hand_landmarks]
    y_coordinates = [landmark.y for landmark in hand_landmarks]

    # print(x_coordinates)
    text_x = int(min(x_coordinates) * width)
    text_y = int(min(y_coordinates) * height) - MARGIN

    # Draw handedness (left or right hand) on the image.
    cv2.putText(annotated_image, f"{handedness[0].category_name}",
                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,
                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)

  return annotated_image

"""Cropping the bbox references to code above"""

from PIL import Image

def crop_detection(rgb_image, detection_result,fname):
  hand_landmarks_list = detection_result.hand_landmarks
  handedness_list = detection_result.handedness

  # Loop through the detected hands to visualize.
  annotated_image = np.copy(rgb_image.numpy_view())
  ans = [];
  margin_err = 0.13
  for idx in range(len(hand_landmarks_list)):
    hand_landmarks = hand_landmarks_list[idx]
    handedness = handedness_list[idx]

    height, width, _ = annotated_image.shape
    x_coordinates=[]
    y_coordinates=[]
    x_coordinates = [landmark.x for landmark in hand_landmarks]
    y_coordinates = [landmark.y for landmark in hand_landmarks]
    # print(min(x_coordinates))
    # print(min(y_coordinates))
    # print(max(x_coordinates))
    # print(max(y_coordinates))
    min_x = (min(x_coordinates) * width)
    min_y = (min(y_coordinates) * height)
    max_x = (max(x_coordinates) * width)
    max_y = (max(y_coordinates) * height)
    #cv2.rectangle(annotated_image, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)
    #cv2_imshow(annotated_image)


    # print(hand_landmarks_list[idx])
    # print(x_coordinates)
    # print(min_x)
    # print(min_y)
    # print(max_x)
    # print(max_y)

    w = max_x-min_x
    h = max_y-min_y
    min_x -= w*margin_err
    min_x = max(0,min_x)
    min_y -= h*margin_err
    min_y = max(0,min_y)
    max_x += w*margin_err
    max_x = min(width,max_x)
    max_y += h*margin_err
    max_y = min(height,max_y)

    # print(annotated_image.shape)
    box = (min_x, min_y, max_x, max_y)
    ans = Image.fromarray(annotated_image)
    ans = ans.crop(box)
    # cv2_imshow(cv2.cvtColor(np.array(ans), cv2.COLOR_BGR2RGB))
    cv2.imwrite("./crop_frame/"+fname+str(idx)+".png", cv2.cvtColor(np.array(ans), cv2.COLOR_BGR2RGB))

"""Cropping out frame"""

# STEP 1: Import the necessary modules.
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision

# STEP 2: Create an HandLandmarker object.
base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')
options = vision.HandLandmarkerOptions(base_options=base_options,
                                       num_hands=10)
detector = vision.HandLandmarker.create_from_options(options)
new_dir = "./output_frame/"
# Load the input image.
imgList = os.listdir(new_dir);
total = 0
correct = 0
imgList.sort()
print(imgList)
shutil.rmtree("./crop_frame/")
os.mkdir("./crop_frame")
for imgName in imgList:
# STEP 3: Load the input image.
  image = mp.Image.create_from_file("./output_frame/"+imgName)#"./new_data/leapGestRecog/00/01_palm/frame_00_01_0003.png")
  imgID=imgName[0:(len(imgName)-4)]
# STEP 4: Detect hand landmarks from the input image.
  detection_result = detector.detect(image)

# STEP 5: Process the classification result. In this case, visualize it.
  crop_detection(image, detection_result,fname=imgID)

"""Import gesture_recognizer"""

from mediapipe_model_maker import gesture_recognizer

"""import dataset and split into three pieces for cross validation"""

# Load the rock-paper-scissor image archive.
data = gesture_recognizer.Dataset.from_folder(
    dirname="./crop_data/",#IMAGES_PATH,
    hparams=gesture_recognizer.HandDataPreprocessingParams()
)

# Split the archive into training, validation and test dataset.
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)

"""Train model"""

# Train the model
hparams = gesture_recognizer.HParams(export_dir="rock_paper_scissors_model", batch_size =8,
    epochs=10,)
options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)
model = gesture_recognizer.GestureRecognizer.create(
    train_data=train_data,
    validation_data=validation_data,
    options=options
)

"""Test model"""

loss, acc = model.evaluate(test_data, batch_size=1)
print(f"Test loss:{loss}, Test accuracy:{acc}")

"""export model"""

# Export the model bundle.
model.export_model()

# Rename the file to be more descriptive.
!mv rock_paper_scissors_model/gesture_recognizer.task rock_paper_scissors.task

"""download model"""

from google.colab import files
files.download("rock_paper_scissors.task")

from google.colab.patches import cv2_imshow
import cv2

"""just another detection test, user can ignore this part"""

# detection test
# ==========================================================================================
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import os

# Create a GestureRecognizer object.
model_path = os.path.abspath("rock_paper_scissors.task")
recognizer = vision.GestureRecognizer.create_from_model_path(model_path)
new_dir = "./rps_data_sample/rock/"
# Load the input image.
imgList = os.listdir(new_dir);
total = 0
correct = 0
for imgName in imgList:
  total+=1
  image = mp.Image.create_from_file(new_dir+imgName)#'rps_data_sample/scissors/117.jpg')#7.jpg')#'rps_data_sample/scissors/100.jpg')

# Run gesture recognition.
  recognition_result = recognizer.recognize(image)
  # print(recognition_result)

# Display the most likely gesture.
  if(recognition_result.gestures == []):continue
  top_gesture = recognition_result.gestures[0][0]
  if(top_gesture.category_name=="fist"):#"fist" or top_gesture.category_name=="like"):
    correct+=1;
print(correct/total)
#print(f"Gesture recognized: {top_gesture.category_name} ({top_gesture.score})")

"""Frame extraction"""

#frame extraction
import time
cap = cv2.VideoCapture('1.mov')
size = (
	int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
	int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
)
# codec = cv2.VideoWriter_fourcc(*'DIVX')
# output = cv2.VideoWriter('human2.avi',codec,25.0,size)
shutil.rmtree("./output_frame/")
os.mkdir("./output_frame")

i = 0
frame_rate_divider = 1
while(cap.isOpened()):
    stime = time.time()
    ret, frame = cap.read()
    i += 1
    #if(i<=150):continue
    if ret:
        if i % frame_rate_divider == 0:
            ms = cap.get(cv2.CAP_PROP_POS_MSEC);
            s = ms/1000;
            s = int(s)
            mins = int(s/60)
            s = s-mins*60
            cv2.imwrite("output_frame/frame"+str(i)+"_"+str(mins)+"min"+str(s)+"s"+".jpg",frame)
            # output.write(frame)
        # print('FPS {:.1f}'.format(1 / (time.time() - stime)))
        if(i==808):break;
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
# output.release()
cv2.destroyAllWindows()

import shutil
shutil.rmtree("./output_frame/")

"""Detection on frame"""

import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import os

# Create a GestureRecognizer object.
model_path = os.path.abspath("rock_paper_scissors.task")
recognizer = vision.GestureRecognizer.create_from_model_path(model_path)
new_dir = "./output_frame/"
# Load the input image.
imgList = os.listdir(new_dir);
imgList.sort()
total = 0
correct = 0
for imgName in imgList:
  total+=1
  image = mp.Image.create_from_file(new_dir+imgName)#'rps_data_sample/scissors/117.jpg')#7.jpg')#'rps_data_sample/scissors/100.jpg')

# Run gesture recognition.
  recognition_result = recognizer.recognize(image)
  # print(recognition_result)

# Display the most likely gesture.
  if(recognition_result.gestures == []):continue
  top_gesture = recognition_result.gestures[0][0]
  if(not (top_gesture.category_name=="none" or top_gesture.category_name=="")):
    print(imgName)
    print(top_gesture.category_name)
# print(correct/total)
#print(f"Gesture recognized: {top_gesture.category_name} ({top_gesture.score})")

