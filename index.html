<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>CS766 2024 Spring Final Project</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.0/font/bootstrap-icons.css">

</head>

<body>
  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex justify-content-between align-items-center">

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#introduction">Introduction</a></li>
          <li><a class="nav-link scrollto" href="#dea">Data Exploratory</a></li>
          <li><a class="nav-link scrollto" href="#segmentation">Segmentation</a></li>
          <li><a class="nav-link scrollto" href="#handgesture">Hand Gesture</a></li>
          <li><a class="nav-link scrollto" href="#keyframe">Keyframe</a></li>
          <li><a class="nav-link scrollto" href="#conclusion">Conclusion</a></li>
          <li><a class="nav-link scrollto" href="#reference">Reference</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="zoom-in" data-aos-delay="100">
      <h1>Exploratory Data Analysis and Baseline Feature Engineering for Educational Video Data</h1>
      <!-- <h2>Gesture Chunk | Segmentation | Hand Gesture Detection | Keyframe Extraction</h2> -->
      <a href="#about" class="btn-get-started" style="margin-top: 100px;">Get Started</a>
    </div>
  </section><!-- End Hero Section -->


  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about">
        <div class="container" data-aos="fade-up">
          <div class="row about-container">
  
            <div class="col-lg-6 content order-lg-1 order-2">
              <h2 class="title">About</h2>
              
              <div class="icon-box" data-aos="fade-up" data-aos-delay="100">
                <div class="icon"><i class="bi-box-fill"></i></div>
                <h4 class="title"><a href="">Course Project</a></h4>
                <p class="description" style="color:#333;">This webpage presents the project report for 
                    the COMP SCI/​ECE 766 COMPUTER VISION course offered during the Spring 2024 semester 
                    at UW Madison, taught by Pedro Morgado.
                </p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="200">
                <div class="icon"><i class="bi-people-fill"></i></div>
                <h4 class="title"><a href="">Teams</a></h4>
                <p class="description" style="color:#333;">Our team members, 
                    <span style="color:rgb(53, 36, 235)">Boyuan Zou</span>,
                    <span style="color:rgb(53, 36, 235)">Zhongling Liao</span>,
                    <span style="color:rgb(53, 36, 235)">Xin Cai</span>, operate under the name 'DKHTNI'. 
                    This acronym can be interpreted in multiple ways, such as 'Databases Keep Huge Tables Neatly Indexed' or 
                    'Dreams Kindle Hope, Transforming New Ideas'.</p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-github"></i></div>
                <h4 class="title"><a href="">Links</a></h4>
                <p class="description" style="color:#333;">This webpage w
                </p>
                
              </div>

              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-bookmark-star-fill"></i></div>
                <h4 class="title"><a href="">Credits</a></h4>
                <p class="description" style="color:#333;">This webpage was created by slightly modifying the
                     <a href="https://bootstrapmade.com/regna-bootstrap-onepage-template/#download">Regna Bootstrap Theme</a> from BOOTSTRAPMADE.
                    Image (Right) courtesy of <a href="https://news.mit.edu/2023/when-computer-vision-works-like-human-brain-0630" target="_blank">MIT News</a>.</p>
                
              </div>
  
            </div>
  
            <div class="col-lg-6 background order-lg-2 order-1" data-aos="fade-left" data-aos-delay="100"></div>
          </div>
  
        </div>
      </section><!-- End About Section -->


    <!-- ======= Problem Statement Section ======= -->
    <section id="introduction">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Introduction</h3>
          <h4 class="subtitle">Project Background</h4>
            <p class="paragraph">This project aims to design tools to help educational researchers 
                analyze video data. A growing body of educational research has demonstrated that gestures 
                can predict mathematical thinking and performance (e.g., Abrahamson et al. 2020). Educational 
                researchers conduct interventions to collect video data capturing students’ gestures as they solve 
                mathematical problems. They focus mainly on identifying specific types of gestures, such as 
                representational and dynamic gestures, which can predict students’ mathematical performance. 
                However, they currently have to manually analyze these videos by repeatedly watching them, which 
                can be very time-consuming and thus hampers the progress of their research. This project responds 
                to the urgent need for reliable and automated analysis tools.
            </p>
          <h4 class="subtitle">Problem Statement</h4>
            <p class="paragraph">Extracting meaningful data from images and videos is a central task in computer 
                vision. This task typically involves building and training machine learning or deep learning models. 
                Specifically, our project targets a set of abstract gestures relevant to the educational domain. While 
                existing tools can detect landmarks of the human body and hands in videos, the representation provided 
                by this low-level data is not directly suitable for model development due to its complexity and abstraction 
                from the high-level concepts needed. Given this, our project focuses on performing exploratory data 
                analysis and establishing a baseline feature layer. This layer will bridge the gap between the low-level 
                landmark data and the high-level requirements necessary for effective gesture analysis.
            </p>
        </div>
      </div>
    </section>

    
    <!-- ======= Exploratory Data Analysis Section ======= -->
    <section id="dea">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Exploratory Data Anlaysis</h3>
          <p class="paragraph">Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque</p>
        </div>
      </div>
    </section>

    
    <!-- Segmentation  -->
    <section id="segmentation">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Segmentation</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                Todo...Introduce methods for segmentation
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                Todo...Display results.
            </p>

            </p>
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                Todo...add comments to current algorithms and results.
            </p>
          </div>
        </div>
    </section>


    <!-- Hand Gesture Detection  -->
    <section id="handgesture">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Hand Gesture Detection</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                Todo...Introduce methods
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                Todo...Display results
            </p>

            </p>
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                Todo...add comments to current algorithms and results.
            </p>
          </div>
        </div>
    </section>


    <!-- Keyframe Extraction  -->
    <section id="keyframe">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Keyframe Extraction</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                A <span style="font-style: italic;">gesture chunk</span> is a continuous segment of frames from the original video data, 
                capturing the animation of gestures in an active stage. The purpose of keyframe extraction is to represent this animation 
                using fewer frames. For simplicity, this baseline keyframe extraction focuses solely on the movement of the arms, ignoring 
                hand movements. Consequently, the landmarks of the wrists (left and right) are critical for selecting keyframes. 
                At a high level, we aim to disregard frames where the wrist’s position remains nearly unchanged. If the wrist moves from 
                position <span style="font-style: italic;">a</span> to <span style="font-style: italic;">b</span> along a roughly straight 
                line, we designate <span style="font-style: italic;">a</span> and <span style="font-style: italic;">b</span> as keyframes 
                and use linear interpolation to compute all intervening frames. To achieve this, we primarily use the velocity and positions 
                of the wrists’ landmarks to filter the keyframes. Initially, we identify frames where the wrist velocity is minimal (near zero). 
                Next, we examine the wrists’ positions for a second round of filtering. After these two stages of filtering on each of the wrist data, 
                we then union the two sets to extract the keyframe sequence for the given gesture chunk.
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                The videos below showcase a result example from our dataset, specifically selected 
                from original data (MAP014). The scenario shows a student answering the question: 
                <i>Are the lengths of the two diagonals of a rectangle the same?</i> In the first half of 
                the video, the student keep raising his arms to represent a rectangle. In the second 
                half, he swings his upper arms back and forth 8 times to depict the diagonals of the 
                rectangle. Video.1, created from the original data, contains 129 frames. It can be 
                clearly seen the student’s whole body micro-movements even though he remains in a fixed position. 

                Video.2 is constructed from 15 extracted keyframes generated by our algorithm, which selects 
                frames 0, 5, 7, 13, 70, 75, 93, 99, 100, 103, 107, 109, 121, 124, and 128 as keyframes. 
                By employing linear interpolation, we reconstruct the other frames. This video utilizes 
                normalized data, where landmarks have been transformed to the fixed shoulder coordinate 
                system, significantly reducing noise from whole body micro-movements. This keyframe 
                extraction process not only reduces the number of frames needed to represent the gesture 
                chunk but also further minimizes noise from micro-movements. For this dataset, the current 
                keyframe extraction algorithm is estimated to extract keyframes amounting to 10% to 20% of 
                the total number of original frames.
                <br><br>
                <span style="color:rgb(115, 93, 176)">
                    <b>Note</b>: If these videos are not playing in Google Chrome, please try using Safari instead.
                </span>
            </p>

    
            <div class="video-container">
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/PRE.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.1 </b></span> Made from 129 original frames.
                    </p>
                </div>
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/KEY.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.2 </b></span> Made from 15 extracted keyframes.
                    </p>
                </div>
            </div>
            
            
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                The baseline algorithm performs adequately for arm-based gestures, as demonstrated by the results provided. However, 
                several potential limitations exist: 1) The algorithm may not effectively track tracing-based gestures, such as a 
                student using their index finger to draw a circle in the air. 2) It does not account for hand gestures, focusing only 
                on arm movements. 3) Linear interpolation may not accurately represent the arc-like movements of the wrists. 4) The issue 
                of feature normalization, particularly how to handle the time gap between each pair of keyframes, remains unexplored. Overall, 
                there are several aspects of this baseline algorithm that could be improved to enhance the robustness of keyframe feature 
                extraction and its suitability for addressing high-level questions.
            </p>
          </div>
        </div>
    </section>


    <!-- ======= Conclusion Section ======= -->
    <section id="conclusion">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Conclusion</h3>
          <h4 class="subtitle">Summary</h4>
            <p class="paragraph">
                In our exploratory data analysis of a portion of the dataset, we focused primarily on the 
                landmarks of shoulders, elbows, and wrists. We have successfully demonstrated the feasibility 
                of transforming these landmarks into a fixed-shoulder coordinate system, which allows for 
                normalization across varying participant-screen distances. Additionally, we discovered that the 
                relative heights of the elbow and wrist serve as effective predictors for distinguishing 
                between ‘idle’ and ‘active’ stages of movement. We introduced the concept of a ‘gesture chunk’ as 
                our unit of analysis, which contains a section of continuous frames within an ‘active’ stage. 
                To support this framework, we developed a baseline algorithm that constructs the fields of the 
                gesture chunk structure: segmentation, hand gesture detection, and keyframe extraction. 
                These components collectively form the feature layer for the dataset, paving the way for 
                further analysis and model development.
            </p>
            <h4 class="subtitle">Future Directions</h4>
            <p class="paragraph">
                Looking ahead, our efforts will focus on two primary objectives. The first is to enhance 
                the robustness and applicability of the current baseline algorithm for constructing a deep 
                learning model. This includes making the segmentation algorithm more capable of incorporating 
                states of low wrist activity and refining the hand gesture detection model to improve accuracy 
                across a broader range of hand gestures. Additionally, we aim to advance keyframe extraction 
                techniques to better handle tracing-based gestures and integrate these improvements with hand 
                gesture detection. Another goal is to expand our analytical framework by including additional 
                variables that will make the gesture chunk structure more relevant for our analysis. The second 
                objective involves developing models for both unsupervised learning, such as clustering, 
                and supervised learning, which will require a more extensive labeled dataset. These models are 
                intended to aid in the identification of high-level abstract gestures, marking a significant step 
                towards achieving our ultimate goal.
            </p>
        </div>
      </div>
    </section>

    <section id="reference" style="margin-bottom: 20%;">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Reference</h3>
            
            <p style="color:#333;">
                Abrahamsen, D., Nathan, M. J., Williams-Pierce, C., Walkington, C., Ottmar, E. R., Soto, H., & Alibali, M. W. (2020). 
                <i>The Future of Embodied Design for Mathematics Teaching and Learning</i>. Frontiers in Education, STEM Education. 
                Retrieved from <a href="https://doi.org/10.3389/feduc.2020.00147" target="_blank">https://doi.org/10.3389/feduc.2020.00147</a>
            </p>



          </div>
        </div>
      </section>

</main>

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>