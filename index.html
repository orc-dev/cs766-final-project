<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>CS766 2024 Spring Final Project</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.0/font/bootstrap-icons.css">

</head>

<body>
  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex justify-content-between align-items-center">

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#introduction">Introduction</a></li>
          <li><a class="nav-link scrollto" href="#dea">Data Exploratory</a></li>
          <li><a class="nav-link scrollto" href="#segmentation">Segmentation</a></li>
          <li><a class="nav-link scrollto" href="#handgesture">Hand Gesture</a></li>
          <li><a class="nav-link scrollto" href="#keyframe">Keyframe</a></li>
          <li><a class="nav-link scrollto" href="#conclusion">Conclusion</a></li>
          <li><a class="nav-link scrollto" href="#reference">Reference</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="zoom-in" data-aos-delay="100">
      <h1>Exploratory Data Analysis and Baseline Feature Engineering for Educational Video Data</h1>
      <!-- <h2>Gesture Chunk | Segmentation | Hand Gesture Detection | Keyframe Extraction</h2> -->
      <a href="#about" class="btn-get-started" style="margin-top: 100px;">Get Started</a>
    </div>
  </section><!-- End Hero Section -->


  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about">
        <div class="container" data-aos="fade-up">
          <div class="row about-container">
  
            <div class="col-lg-6 content order-lg-1 order-2">
              <h2 class="title">About</h2>
              
              <div class="icon-box" data-aos="fade-up" data-aos-delay="100">
                <div class="icon"><i class="bi-box-fill"></i></div>
                <h4 class="title"><a href="">Course Project</a></h4>
                <p class="description" style="color:#333;">This webpage presents the project report for 
                    the COMP SCI/​ECE 766 COMPUTER VISION course offered during the Spring 2024 semester 
                    at UW Madison, taught by Pedro Morgado.
                </p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="200">
                <div class="icon"><i class="bi-people-fill"></i></div>
                <h4 class="title"><a href="">Teams</a></h4>
                <p class="description" style="color:#333;">Our team members, 
                    <span style="color:rgb(53, 36, 235)">Boyuan Zou</span>,
                    <span style="color:rgb(53, 36, 235)">Zhongling Liao</span>,
                    <span style="color:rgb(53, 36, 235)">Xin Cai</span>, operate under the name 'DKHTNI'. 
                    This acronym can be interpreted in multiple ways, such as 'Databases Keep Huge Tables Neatly Indexed' or 
                    'Dreams Kindle Hope, Transforming New Ideas'.</p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-github"></i></div>
                <h4 class="title"><a href="">Links</a></h4>
                <p class="description" style="color:#333;">
                    Github repository:
                    [<a href="https://github.com/orc-dev/cs766-final-project/tree/main">repo</a>]
                    <br>
                    Presentation slides:
                    [<a href="https://github.com/orc-dev/cs766-final-project/blob/main/presentation/presentation_slides.pdf">pdf</a>]
                    [<a href="https://github.com/orc-dev/cs766-final-project/blob/main/presentation/CS766_PPT.pptx">pptx</a>]
                </p>
                
              </div>

              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-bookmark-star-fill"></i></div>
                <h4 class="title"><a href="">Credits</a></h4>
                <p class="description" style="color:#333;">This webpage was created by slightly modifying the
                     <a href="https://bootstrapmade.com/regna-bootstrap-onepage-template/#download">Regna Bootstrap Theme</a> from BOOTSTRAPMADE.
                    Image (Right) courtesy of <a href="https://news.mit.edu/2023/when-computer-vision-works-like-human-brain-0630" target="_blank">MIT News</a>.</p>
              </div>
            </div>
  
            <div class="col-lg-6 background order-lg-2 order-1" data-aos="fade-left" data-aos-delay="100"></div>
          </div>
  
        </div>
      </section><!-- End About Section -->


    <!-- ======= Problem Statement Section ======= -->
    <section id="introduction">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Introduction</h3>
          <h4 class="subtitle">Motivation</h4>
            <p class="paragraph">This project aims to design tools to help educational researchers 
                analyze video data. A growing body of educational research has demonstrated that gestures 
                can predict mathematical thinking and performance (e.g., Abrahamson et al. 2020). Educational 
                researchers conduct interventions to collect video data capturing students’ gestures as they solve 
                mathematical problems. They focus mainly on identifying specific types of gestures, such as 
                representational and dynamic gestures, which can predict students’ mathematical performance. 
                However, they currently have to manually analyze these videos by repeatedly watching them, which 
                can be very time-consuming and thus hampers the progress of their research. This project responds 
                to the urgent need for reliable and automated analysis tools.
            </p>
          <h4 class="subtitle">Problem Statement</h4>
            <p class="paragraph">Extracting meaningful data from images and videos is a central task in computer 
                vision. This task typically involves building and training machine learning or deep learning models. 
                Specifically, our project targets a set of abstract gestures relevant to the educational domain. While 
                existing tools can detect landmarks of the human body and hands in videos, the representation provided 
                by this low-level data is not directly suitable for model development due to its complexity and abstraction 
                from the high-level concepts needed. Given this, our project focuses on performing exploratory data 
                analysis and establishing a baseline feature layer. This layer will bridge the gap between the low-level 
                landmark data and the high-level requirements necessary for effective gesture analysis.
            </p>
        </div>
      </div>
    </section>

    
    <!-- ======= Exploratory Data Analysis Section ======= -->
    <section id="dea">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Exploratory Data Anlaysis</h3>
          <h4 class="subtitle">Low-Level Data on Body and Hand Landmark Coordinates</h4>
            <p class="paragraph">We utilize OpenCV and MediaPipe to annotate original videos with body and hand landmarks, 
                resulting in new videos that display these landmarks. Additionally, we store the coordinates of these 
                landmarks in CSV files as raw data. Below are the configurations from MediaPipe for the landmarks 
                (Left: 
                <a href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker">Pose Landmarker Model</a>. 
                Middle: 
                <a href="https://developers.google.com/mediapipe/solutions/vision/hand_landmarker">Keypoint Localization of 21 Hand-knuckle</a>. 
                Right: An example of screenshot of generated video)
            </p>
            
            <div class="image-container" style="background-color: rgb(227, 233, 235);">
                <img src="assets/img/pose_landmarks_index.png" alt="pose landmarks index" 
                    style="max-height: 300px; width: auto; margin:10px; margin-top: 25px; margin-bottom: 25px;">
                <img src="assets/img/hand_landmarks_index.png" alt="hand landmarks index" 
                    style="max-height: 200px; width: auto; margin:10px">
                <img src="assets/img/landmark_example.png" alt="Description of Image 3"
                style="max-height: 300px; width: auto; margin: 10px; margin-top: 25px; margin-bottom: 25px;">
            </div>

            
            <p class="paragraph">
                Upon reviewing the landmark-highlighted videos, we discovered that some landmarks, particularly 
                those on the hands, are occasionally undetected by MediaPipe. There are also instances where the 
                positions of the landmarks are incorrectly identified. Unfortunately, these errors tend to occur more 
                frequently when participants are making gestures. We estimate that at least 15% of the data from this 
                phase may be missing or incorrect.
            </p>

            <h4 class="subtitle">Insights from Data Exploration of the Shoulder, Elbow, and Wrist</h4>
            <p class="paragraph">
                MediaPipe has identified over 100 landmarks in our dataset; however, we are primarily interested in 
                a relatively small subset of these landmarks. The experimental setup involves participants standing 
                alone in a fixed position, facing the camera, and performing gestures. We observed that in less than 
                2% of the cases, participants turn their bodies or bend down. Consequently, we have chosen to disregard 
                these non-standing frames and focus solely on those where the participant is standing and facing the 
                camera. We found that the most informative movements are expressed through the arms and hands. Based 
                on these observations, we have anchored the shoulder landmarks at two fixed points and normalized the 
                remaining data to this fixed-shoulder coordinate system. Testing shows that this normalization 
                significantly reduces noise caused by subtle whole-body movements. Additionally, it minimizes 
                variations in participant distance from the camera, allowing for a more consistent comparison of arm 
                movements across different participants.
            </p>
            <img src="assets/img/MAP014_elbow_wrist_landmarks.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">
            <p class="paragraph">
                One observation from our study is that participants tend to alternate between ‘idle’ and ‘active’ stages. 
                During ‘idle’ stages, participants generally stand alone in a relaxed manner, either reading the prompt 
                or contemplating, with minimal gestural activity. In contrast, the ‘active’ stages are marked by an increase 
                in gestures as participants engage in solving math problems. This segmentation effectively serves as 
                a ‘separator’, allowing us to transform the continuous video stream into discrete units that capture the 
                phases we’re most interested in. Further analysis of the elbow and wrist landmarks illuminates the distinction 
                between these two stages. The image above includes a plot visualizing the frequency of elbow and wrist 
                landmarks: the two yellow dots represent fixed shoulder positions, light blue and red dots indicate elbow 
                landmarks, and dark blue and red dots represent wrist landmarks. Clearly, there is a discernible pattern in 
                the distribution of elbow and wrist data. A useful heuristic insight is that the relative heights of the wrist 
                and elbow can help identify the stages: in the ‘idle’ stage, the wrist typically appears lower than the elbow, 
                whereas in the ‘active’ stage, the wrist tends to be higher. This data exploration has led us to propose the 
                concept of a ‘gesture chunk’ as the unit of analysis, which we will introduce in the following section.
            </p>


            <h4 class="subtitle">'Gesture Chunk' as a New Unit of Analysis in Abstraction</h4>
            <p class="paragraph">
                In our exploratory data analysis, we introduce the concept of a ‘gesture chunk’ as the unit of 
                analysis for this dataset. Observations show that students alternate between ‘idle’ and ‘active’ 
                states during video recordings. We define a gesture chunk as a segment of body movement occurring 
                during an active state, making it a distinguishable and countable unit that comprises a continuous 
                section of frames within the original video.

                To facilitate our analysis, we propose constructing feature fields within the gesture chunk structure. 
                The first feature involves segmentation: identifying each ‘idle’ and ‘active’ state to determine the 
                bounding frames for each gesture chunk. This segmentation allows us to pinpoint all gesture chunks 
                in a video file. Additionally, we can compute segment-related features such as the starting frame 
                ID, starting timestamp, ending frame ID, ending timestamp, and duration.
                
                To simplify our feature engineering process, we separate the analysis of hand gestures from arm 
                gestures. For hand gesture detection, the second type of feature, we plan to utilize existing models 
                to identify the gestures of each hand in every frame of a gesture chunk.
                
                The final feature we focus on is keyframe extraction, akin to the reverse process of creating an 
                animation by setting keyframes. This feature involves selecting a subset of frames within a gesture 
                chunk, primarily focusing on the motion of the wrist landmarks. Although additional features may be 
                considered in future developments, in this project, our focus is strictly on establishing baseline 
                algorithms for segmentation, hand gesture detection, and keyframe extraction.
            </p>
        </div>
      </div>
    </section>

    
    <!-- Segmentation  -->
    <section id="segmentation">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Segmentation</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                Temporal segmentation serves as a vital preprocessing step in various analytical tasks, including 
                key frame extraction and gesture recognition. While existing literature presents diverse segmentation 
                techniques tailored to specific data formats, not all are compatible with our current dataset. 
                After thorough review, we identified several studies that, with modifications, align closely with our 
                project objectives.

                In [2], [3], and [4], Support Vector Machine (SVM) classifiers were utilized to distinguish between 
                rest and gesture frames. [1], on the other hand, quantified frame movement by summing pixel changes 
                beyond a certain threshold, employing a heuristic to classify frames accordingly. Additionally, 
                [5] detected static hand positions and compared them to real-time hand positions at each time point.

                Drawing inspiration from these approaches, we developed a compound algorithm that integrates static 
                position analysis, relative distance calculation, and local movement quantification. The following
                 presents a structured overview of our main algorithm, comprising four distinct steps:
            </p>

            <img src="assets/img/segment_img0.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">
            
            <p class="paragraph"><strong>Algorithm 1: Preliminary Static Positions</strong><br>
            <b>Input</b>: cell_num, std_data<br>
            1. Divide each frame into a grid of dimensions cell_num by cell_num and compute the frequency of the left 
            elbow's position within each grid cell.<br>
            2. Identify the cell with the highest frequency as the static position.<br>
            3. Determine time periods during which the elbow remains in the static position.<br>
            <b>Output</b>: static position indicator vector<br>
            <br>
            <strong>Algorithm 2: Quantity of Movement Segmentation</strong><br>
            <b>Input</b>: std_data, delta<br>
            1. Compute local velocities using the Euclidean norm of body landmark movements over time.<br>
            2. Calculate the mean and standard deviation of local velocities during preliminary static time periods.<br>
            3. Set a threshold as mean + delta * standard deviation to classify time periods with velocities below the threshold as rest.<br>
            <b>Output</b>: rest position indicator vector<br>
            <br>
            <strong>Algorithm 3: Elbow Relative Distance Segmentation</strong><br>
            <b>Input</b>: std_data, delta<br>
            1. Compute the Euclidean norm from the current elbow position to the static position as the relative distance.<br>
            2. Determine the mean and standard deviation of relative distances during preliminary static time periods.<br>
            3. Classify time periods with distances below the threshold of mean + delta * standard deviation as rest.<br>
            <b>Output</b>: rest position indicator vector<br>
            <br>
            <strong>Algorithm 4: Smoothing</strong><br>
            <b>Input</b>: an indicator vector, L, r<br>
            1. Employ a sliding window of length L to compute the ratio of 1s in the vicinity of each time point.<br>
            2. Identify time points where the ratio exceeds r as rest positions.<br>
            <b>Output</b>: A smoothed indicator vector<br>
            <br>
            <strong>Main Algorithm:</strong><br>
            <b>Input</b>: std_data, L, r, delta, cell_num<br>
            1. Execute Algorithm 1 to obtain the preliminary static position indicator vector.<br>
            2. Utilize Algorithms 2 and 3 with appropriate parameters to generate indicator vectors.<br>
            3. Compute the intersection vector of the indicators 2.<br>
            4. Apply Algorithm 4 to smooth the intersection vector.<br>
            <b>Output</b>: The rest time indicator vector<br>
            </p>
            
            <h4 class="subtitle">Results</h4>

            <div class="image-grid">
                <img src="assets/img/seg_map040.png" alt="map040">
                <img src="assets/img/seg_map108.png" alt="map108">
                <img src="assets/img/seg_map064.png" alt="map064">
                <img src="assets/img/seg_map046.png" alt="map046">
            </div>

            <p class="paragraph">
                The four plots above showcase the segmentation results for four randomly selected videos. 
                In each plot, the dark blue points denote the heights of the left elbow of the object, while the 
                light blue points represent the heights of the left wrist. Time periods highlighted in yellow 
                denote segments classified as rest, while the remaining intervals, excluding the initial and final 
                durations, are categorized as containing gestures.
                It is important to note that our algorithm is unable to classify time periods devoid of data, 
                wherein the heights of both the elbow and wrist are recorded as 0. This limitation is consistent 
                with the constraints discussed in the subsequent section.
            </p>
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                The effectiveness of temporal segmentation in our project is hindered by the inherent challenges 
                posed by our unlabeled dataset. Originating from raw data sourced from education science research, 
                our dataset lacks ground truth segmentation labels and remains unfiltered for noise. Consequently, 
                assessing the accuracy of our segmentation relies primarily on subjective evaluation through visual 
                inspection or manual labeling, both of which are labor-intensive tasks. Regrettably, resource constraints 
                prevent us from annotating the entire dataset comprehensively. Thus, the parameters utilized in our 
                algorithm are determined empirically through iterative experimentation, as opposed to being optimized 
                through supervised learning with labeled data.
                
                Moreover, our algorithm encounters limitations associated with the quality of the data itself. 
                Landmarks extracted from MediaPipe exhibit instability, leading to erroneous labeling of certain 
                landmarks as 0 due to model instability. To mitigate this issue, our algorithm leverages a subset 
                of the available landmark information. However, instances where location information is unavailable 
                pose a significant challenge, and our algorithm lacks the capability to discern between rest and 
                gesture during these timepoints.
            </p>
          </div>
        </div>
    </section>


    <!-- Hand Gesture Detection  -->
    <section id="handgesture">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Hand Gesture Detection</h3>
            <h4 class="subtitle">Reason</h4>
            <p class="paragraph">
                Since we want to extract the active gesture chunk, the gesture detection of the video can be a 
                useful tool for help. We plan to train a gesture detection model and then use it to detect if 
                there is any gesture showing up in the video. If some gesture detected is in our target gesture 
                class, we can safely assume that this period in the video where the gesture is detected is an 
                active gesture chunk.
            </p>

            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                We have seen in previous tasks of hand detection using Mediapipe. We will continuously use Mediapipe 
                for gesture recognition. We first cut the video into multiple frames.
                <br><br>
                <img src="assets/img/1.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">

                And use hand detection to get the min and max x,y coordinates of the hand key point of hand which 
                forms a bounding box of hand and then crop out this region with some margin error. 
                <br><br>
                <img src="assets/img/2.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">

                Then, we save this cropped part in a directory. Then, we find some hand gesture detection dataset, 
                which will be discussed in more detail later in the result part. And then, we use transfer 
                learning to train a new model in the media pipe maker. Code references its 
                <a href="https://youtu.be/vODSFXEP-XY?si=Hdd0u3vKnF7kLur-">official tutorial</a>:
                <br><br>
                <img src="assets/img/3.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">

                Then, we use the trained media pipe model to detect each cropped frame and then output if each 
                frame has some detected gesture other than empty or none, we put a label on this frame.
                <img src="assets/img/4.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">

            </p>

            <h4 class="subtitle">Results and Limitations</h4>
            <p class="paragraph">
                Although method is relatively simple and clearly, after we actually deployed this method, we found a lot of 
                problems. First, because the backbone used by the media pipe is tensorflow, and tensorflow stopped supporting 
                Windows Operating System a long time ago, we have to train and test on the google colab service. This is a free 
                Linux platform for training some simple models, thus if we try to load it with a very large dataset, for example, 
                for the dataset more than 10000 images, it will stop working. If a more advanced platform is provided, 
                we may provide a better result.
                <br><br>
                Second, we find out that, crop out hand from each frame for detection can cause problem, due to the low quality 
                of video itself. Some hand will not be detected by the hand detection pretrained model provided by 
                official site, but it’s gesture will be detected directly from our trained model. Thus, we give up cropping 
                first and then detect. We directly use each frame for detection until we have a high quality video.

                Due to such limitations, we only include 4 classes+1 none class. They are:fist, like(thumb in ppt), 
                one(indexing in ppt), palm. The dataset we used is a light version of HaGRID 
                (<a href="https://github.com/hukenovs/hagrid">HAnd Gesture Recognition 
                    Image Dataset</a>), to be more precise is ”hagrid-sample-30k-384p”, 
                combining with official rock_paper_scissors dataset(you can get it from the official tutorial video above. 
                We do a few tests of training, with different parameters and dataset and get different results. For either 
                type of dataset combination we choose, we use a simple version of cross validation. We put 80% of the dataset 
                for training, 10% for validation, and the last 10% for testing. For all of different combinations of dataset, 
                and different parameter, we have a test accuracy more than 95%:
                <br><br>
                <img src="assets/img/5.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">
                <br>
                However, in reality, when we actually put it in the video to detect it, the results weren't very 
                promising. We found that without using a hybrid training set, the individual training sets were 
                heavily biased. For example, even if we use 1400 images for each class from Hagrid Dataset for training, 
                the result in multiple cases is worse than using 200 images from Hagrid Dataset and 125 from Official 
                dataset after training in the same epoch. This might be caused by the quality of the dataset itself. 
                For example, for frame 728 and following, it should obviously be palm instead of like. 
                <br><br>
                <img src="assets/img/6.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">
                
                All the different parameter pre-trained models are provided on github, and you are welcome to 
                test those models, but “gesture 200dataHagrid100Official 10 epoch.task” is a recommended model. 
                Some other limitations are the light-weighted Media Pipe model itself. The only parameter allowed 
                to be changed is epoch and batch size along with the numbers of fully connected layers and its 
                active layers. No other architecture is provided for further fine tuning. We have two further directions,
                first seeking high quality video. Current video has such low resolution that in many frames, hands 
                are blurred and hard to detect. Second, train a better model. Find more dataset and try to combine 
                those dataset so we can limit the bias to as small as possible. Moreover, find a better model 
                structure and a better platform to train it.
            </p>

          </div>
        </div>
    </section>


    <!-- Keyframe Extraction  -->
    <section id="keyframe">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Keyframe Extraction</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                A <span style="font-style: italic;">gesture chunk</span> is a continuous segment of frames from the original video data, 
                capturing the animation of gestures in an active stage. The purpose of keyframe extraction is to represent this animation 
                using fewer frames. For simplicity, this baseline keyframe extraction focuses solely on the movement of the arms, ignoring 
                hand movements. Consequently, the landmarks of the wrists (left and right) are critical for selecting keyframes. 
                At a high level, we aim to disregard frames where the wrist’s position remains nearly unchanged. If the wrist moves from 
                position <span style="font-style: italic;">a</span> to <span style="font-style: italic;">b</span> along a roughly straight 
                line, we designate <span style="font-style: italic;">a</span> and <span style="font-style: italic;">b</span> as keyframes 
                and use linear interpolation to compute all intervening frames. To achieve this, we primarily use the velocity and positions 
                of the wrists’ landmarks to filter the keyframes. Initially, we identify frames where the wrist velocity is minimal (near zero). 
                Next, we examine the wrists’ positions for a second round of filtering. After these two stages of filtering on each of the wrist data, 
                we then union the two sets to extract the keyframe sequence for the given gesture chunk.
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                The videos below showcase a result example from our dataset, specifically selected 
                from original data (MAP014). The scenario shows a student answering the question: 
                <i>Are the lengths of the two diagonals of a rectangle the same?</i> In the first half of 
                the video, the student keep raising his arms to represent a rectangle. In the second 
                half, he swings his upper arms back and forth 8 times to depict the diagonals of the 
                rectangle. Video.1, created from the original data, contains 129 frames. It can be 
                clearly seen the student’s whole body micro-movements even though he remains in a fixed position. 

                Video.2 is constructed from 15 extracted keyframes generated by our algorithm, which selects 
                frames 0, 5, 7, 13, 70, 75, 93, 99, 100, 103, 107, 109, 121, 124, and 128 as keyframes. 
                By employing linear interpolation, we reconstruct the other frames. This video utilizes 
                normalized data, where landmarks have been transformed to the fixed shoulder coordinate 
                system, significantly reducing noise from whole body micro-movements. This keyframe 
                extraction process not only reduces the number of frames needed to represent the gesture 
                chunk but also further minimizes noise from micro-movements. For this dataset, the current 
                keyframe extraction algorithm is estimated to extract keyframes amounting to 10% to 20% of 
                the total number of original frames.
                <br><br>
                <span style="color:rgb(115, 93, 176)">
                    <b>Note</b>: If these videos are not playing in Google Chrome, please try using Safari instead.
                </span>
            </p>

    
            <div class="video-container">
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/PRE.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.1 </b></span> Made from 129 original frames.
                    </p>
                </div>
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/KEY.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.2 </b></span> Made from 15 extracted keyframes.
                    </p>
                </div>
            </div>
            
            
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                The baseline algorithm performs adequately for arm-based gestures, as demonstrated by the results provided. However, 
                several potential limitations exist: 1) The algorithm may not effectively track tracing-based gestures, such as a 
                student using their index finger to draw a circle in the air. 2) It does not account for hand gestures, focusing only 
                on arm movements. 3) Linear interpolation may not accurately represent the arc-like movements of the wrists. 4) The issue 
                of feature normalization, particularly how to handle the time gap between each pair of keyframes, remains unexplored. Overall, 
                there are several aspects of this baseline algorithm that could be improved to enhance the robustness of keyframe feature 
                extraction and its suitability for addressing high-level questions.
            </p>
          </div>
        </div>
    </section>


    <!-- ======= Conclusion Section ======= -->
    <section id="conclusion">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Conclusion</h3>
          <h4 class="subtitle">Summary</h4>
            <p class="paragraph">
                In our exploratory data analysis of a portion of the dataset, we focused primarily on the 
                landmarks of shoulders, elbows, and wrists. We have successfully demonstrated the feasibility 
                of transforming these landmarks into a fixed-shoulder coordinate system, which allows for 
                normalization across varying participant-screen distances. Additionally, we discovered that the 
                relative heights of the elbow and wrist serve as effective predictors for distinguishing 
                between ‘idle’ and ‘active’ stages of movement. We introduced the concept of a ‘gesture chunk’ as 
                our unit of analysis, which contains a section of continuous frames within an ‘active’ stage. 
                To support this framework, we developed a baseline algorithm that constructs the fields of the 
                gesture chunk structure: segmentation, hand gesture detection, and keyframe extraction. 
                These components collectively form the feature layer for the dataset, paving the way for 
                further analysis and model development.
            </p>
            <h4 class="subtitle">Future Directions</h4>
            <p class="paragraph">
                Looking ahead, our efforts will focus on two primary objectives. The first is to enhance 
                the robustness and applicability of the current baseline algorithm for constructing a deep 
                learning model. This includes making the segmentation algorithm more capable of incorporating 
                states of low wrist activity and refining the hand gesture detection model to improve accuracy 
                across a broader range of hand gestures. Additionally, we aim to advance keyframe extraction 
                techniques to better handle tracing-based gestures and integrate these improvements with hand 
                gesture detection. Another goal is to expand our analytical framework by including additional 
                variables that will make the gesture chunk structure more relevant for our analysis. The second 
                objective involves developing models for both unsupervised learning, such as clustering, 
                and supervised learning, which will require a more extensive labeled dataset. These models are 
                intended to aid in the identification of high-level abstract gestures, marking a significant step 
                towards achieving our ultimate goal.
            </p>
        </div>
      </div>
    </section>

    <section id="reference" style="margin-bottom: 5%;">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Reference</h3>
            <br>
            <p style="color:#333;">
                [0] Abrahamsen, D., Nathan, M. J., Williams-Pierce, C., Walkington, C., Ottmar, E. R., Soto, H., & Alibali, M. W. (2020). 
                <i>The Future of Embodied Design for Mathematics Teaching and Learning</i>. Frontiers in Education, STEM Education. 
                Retrieved from <a href="https://doi.org/10.3389/feduc.2020.00147" target="_blank">https://doi.org/10.3389/feduc.2020.00147</a>
            </p>
            <p style="color:#333;">
                [1] Jiang, Feng, et al. (2015). 
                <i>Multi-layered gesture recognition with Kinect</i>. J. Mach. Learn. Res., 16(1), 227-254. 
                Retrieved from <a href="https://doi.org/yourDOIhere" target="_blank">https://doi.org/yourDOIhere</a>
            </p>
            <p style="color:#333;">
                [2] Ramakrishnan, Ajay Sundar, and Michael Neff. (2013). 
                <i>Segmentation of hand gestures using motion capture data</i>. Proceedings of the 2013 international conference 
                on Autonomous agents and multi-agent systems. 
                Retrieved from <a href="https://doi.org/yourDOIhere" target="_blank">https://doi.org/yourDOIhere</a>
            </p>
            <p style="color:#333;">
                [3] Madeo, Renata CB, Clodoaldo AM Lima, and Sarajane M. Peres. (2013). 
                <i>Gesture unit segmentation using support vector machines: segmenting gestures from rest positions</i>. 
                Proceedings of the 28th Annual ACM Symposium on Applied Computing. 
                Retrieved from <a href="https://doi.org/yourDOIhere" target="_blank">https://doi.org/yourDOIhere</a>
            </p>
            <p style="color:#333;">
                [4] Madeo, Renata Cristina Barros, Sarajane Marques Peres, and Clodoaldo Aparecido de Moraes Lima. (2016). 
                <i>Gesture phase segmentation using support vector machines</i>. Expert Systems with Applications, 56, 100-115. 
                Retrieved from <a href="https://doi.org/yourDOIhere" target="_blank">https://doi.org/yourDOIhere</a>
            </p>
            <p style="color:#333;">
                [5] Peng, Xiaojiang, et al. (2014). 
                <i>Action and gesture temporal spotting with super vector representation</i>. European Conference 
                on Computer Vision. Cham: Springer International Publishing. 
                Retrieved from <a href="https://doi.org/yourDOIhere" target="_blank">https://doi.org/yourDOIhere</a>
            </p>
          </div>
        </div>
      </section>

</main>

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>