<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>CS766 2024 Spring Final Project</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.0/font/bootstrap-icons.css">

</head>

<body>
  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center header-transparent">
    <div class="container d-flex justify-content-between align-items-center">

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#introduction">Introduction</a></li>
          <li><a class="nav-link scrollto" href="#dea">Data Exploratory</a></li>
          <li><a class="nav-link scrollto" href="#segmentation">Segmentation</a></li>
          <li><a class="nav-link scrollto" href="#handgesture">Hand Gesture</a></li>
          <li><a class="nav-link scrollto" href="#keyframe">Keyframe</a></li>
          <li><a class="nav-link scrollto" href="#conclusion">Conclusion</a></li>
          <li><a class="nav-link scrollto" href="#reference">Reference</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="zoom-in" data-aos-delay="100">
      <h1>Exploratory Data Analysis and Baseline Feature Engineering for Educational Video Data</h1>
      <!-- <h2>Gesture Chunk | Segmentation | Hand Gesture Detection | Keyframe Extraction</h2> -->
      <a href="#about" class="btn-get-started" style="margin-top: 100px;">Get Started</a>
    </div>
  </section><!-- End Hero Section -->


  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about">
        <div class="container" data-aos="fade-up">
          <div class="row about-container">
  
            <div class="col-lg-6 content order-lg-1 order-2">
              <h2 class="title">About</h2>
              
              <div class="icon-box" data-aos="fade-up" data-aos-delay="100">
                <div class="icon"><i class="bi-box-fill"></i></div>
                <h4 class="title"><a href="">Course Project</a></h4>
                <p class="description" style="color:#333;">This webpage presents the project report for 
                    the COMP SCI/​ECE 766 COMPUTER VISION course offered during the Spring 2024 semester 
                    at UW Madison, taught by Pedro Morgado.
                </p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="200">
                <div class="icon"><i class="bi-people-fill"></i></div>
                <h4 class="title"><a href="">Teams</a></h4>
                <p class="description" style="color:#333;">Our team members, 
                    <span style="color:rgb(53, 36, 235)">Boyuan Zou</span>,
                    <span style="color:rgb(53, 36, 235)">Zhongling Liao</span>,
                    <span style="color:rgb(53, 36, 235)">Xin Cai</span>, operate under the name 'DKHTNI'. 
                    This acronym can be interpreted in multiple ways, such as 'Databases Keep Huge Tables Neatly Indexed' or 
                    'Dreams Kindle Hope, Transforming New Ideas'.</p>
              </div>
  
              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-github"></i></div>
                <h4 class="title"><a href="">Links</a></h4>
                <p class="description" style="color:#333;">
                    Github repository:
                    [<a href="https://github.com/orc-dev/cs766-final-project/tree/main">repo</a>]
                    <br>
                    Presentation slides:
                    [<a href="https://github.com/orc-dev/cs766-final-project/blob/main/presentation/presentation_slides.pdf">pdf</a>]
                    [<a href="https://github.com/orc-dev/cs766-final-project/blob/main/presentation/CS766_PPT.pptx">pptx</a>]
                </p>
                
              </div>

              <div class="icon-box" data-aos="fade-up" data-aos-delay="300">
                <div class="icon"><i class="bi-bookmark-star-fill"></i></div>
                <h4 class="title"><a href="">Credits</a></h4>
                <p class="description" style="color:#333;">This webpage was created by slightly modifying the
                     <a href="https://bootstrapmade.com/regna-bootstrap-onepage-template/#download">Regna Bootstrap Theme</a> from BOOTSTRAPMADE.
                    Image (Right) courtesy of <a href="https://news.mit.edu/2023/when-computer-vision-works-like-human-brain-0630" target="_blank">MIT News</a>.</p>
                
              </div>
  
            </div>
  
            <div class="col-lg-6 background order-lg-2 order-1" data-aos="fade-left" data-aos-delay="100"></div>
          </div>
  
        </div>
      </section><!-- End About Section -->


    <!-- ======= Problem Statement Section ======= -->
    <section id="introduction">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Introduction</h3>
          <h4 class="subtitle">Motivation</h4>
            <p class="paragraph">This project aims to design tools to help educational researchers 
                analyze video data. A growing body of educational research has demonstrated that gestures 
                can predict mathematical thinking and performance (e.g., Abrahamson et al. 2020). Educational 
                researchers conduct interventions to collect video data capturing students’ gestures as they solve 
                mathematical problems. They focus mainly on identifying specific types of gestures, such as 
                representational and dynamic gestures, which can predict students’ mathematical performance. 
                However, they currently have to manually analyze these videos by repeatedly watching them, which 
                can be very time-consuming and thus hampers the progress of their research. This project responds 
                to the urgent need for reliable and automated analysis tools.
            </p>
          <h4 class="subtitle">Problem Statement</h4>
            <p class="paragraph">Extracting meaningful data from images and videos is a central task in computer 
                vision. This task typically involves building and training machine learning or deep learning models. 
                Specifically, our project targets a set of abstract gestures relevant to the educational domain. While 
                existing tools can detect landmarks of the human body and hands in videos, the representation provided 
                by this low-level data is not directly suitable for model development due to its complexity and abstraction 
                from the high-level concepts needed. Given this, our project focuses on performing exploratory data 
                analysis and establishing a baseline feature layer. This layer will bridge the gap between the low-level 
                landmark data and the high-level requirements necessary for effective gesture analysis.
            </p>
        </div>
      </div>
    </section>

    
    <!-- ======= Exploratory Data Analysis Section ======= -->
    <section id="dea">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Exploratory Data Anlaysis</h3>
          <h4 class="subtitle">Low-Level Data on Body and Hand Landmark Coordinates</h4>
            <p class="paragraph">We utilize OpenCV and MediaPipe to annotate original videos with body and hand landmarks, 
                resulting in new videos that display these landmarks. Additionally, we store the coordinates of these 
                landmarks in CSV files as raw data. Below are the configurations from MediaPipe for the landmarks 
                (Left: 
                <a href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker">Pose Landmarker Model</a>. 
                Middle: 
                <a href="https://developers.google.com/mediapipe/solutions/vision/hand_landmarker">Keypoint Localization of 21 Hand-knuckle</a>. 
                Right: An example of screenshot of generated video)
            </p>
            
            <div class="image-container" style="background-color: rgb(227, 233, 235);">
                <img src="assets/img/pose_landmarks_index.png" alt="pose landmarks index" 
                    style="max-height: 300px; width: auto; margin:10px; margin-top: 25px; margin-bottom: 25px;">
                <img src="assets/img/hand_landmarks_index.png" alt="hand landmarks index" 
                    style="max-height: 200px; width: auto; margin:10px">
                <img src="assets/img/landmark_example.png" alt="Description of Image 3"
                style="max-height: 300px; width: auto; margin: 10px; margin-top: 25px; margin-bottom: 25px;">
            </div>

            
            <p class="paragraph">
                Upon reviewing the landmark-highlighted videos, we discovered that some landmarks, particularly 
                those on the hands, are occasionally undetected by MediaPipe. There are also instances where the 
                positions of the landmarks are incorrectly identified. Unfortunately, these errors tend to occur more 
                frequently when participants are making gestures. We estimate that at least 15% of the data from this 
                phase may be missing or incorrect.
            </p>

            <h4 class="subtitle">Insights from Data Exploration of the Shoulder, Elbow, and Wrist</h4>
            <p class="paragraph">
                MediaPipe has identified over 100 landmarks in our dataset; however, we are primarily interested in 
                a relatively small subset of these landmarks. The experimental setup involves participants standing 
                alone in a fixed position, facing the camera, and performing gestures. We observed that in less than 
                2% of the cases, participants turn their bodies or bend down. Consequently, we have chosen to disregard 
                these non-standing frames and focus solely on those where the participant is standing and facing the 
                camera. We found that the most informative movements are expressed through the arms and hands. Based 
                on these observations, we have anchored the shoulder landmarks at two fixed points and normalized the 
                remaining data to this fixed-shoulder coordinate system. Testing shows that this normalization 
                significantly reduces noise caused by subtle whole-body movements. Additionally, it minimizes 
                variations in participant distance from the camera, allowing for a more consistent comparison of arm 
                movements across different participants.
            </p>
            <img src="assets/img/MAP014_elbow_wrist_landmarks.png" alt="example" 
                style="max-height: 500px; width: auto; display: block; margin: 10px auto;">
            <p class="paragraph">
                One observation from our study is that participants tend to alternate between ‘idle’ and ‘active’ stages. 
                During ‘idle’ stages, participants generally stand alone in a relaxed manner, either reading the prompt 
                or contemplating, with minimal gestural activity. In contrast, the ‘active’ stages are marked by an increase 
                in gestures as participants engage in solving math problems. This segmentation effectively serves as 
                a ‘separator’, allowing us to transform the continuous video stream into discrete units that capture the 
                phases we’re most interested in. Further analysis of the elbow and wrist landmarks illuminates the distinction 
                between these two stages. The image above includes a plot visualizing the frequency of elbow and wrist 
                landmarks: the two yellow dots represent fixed shoulder positions, light blue and red dots indicate elbow 
                landmarks, and dark blue and red dots represent wrist landmarks. Clearly, there is a discernible pattern in 
                the distribution of elbow and wrist data. A useful heuristic insight is that the relative heights of the wrist 
                and elbow can help identify the stages: in the ‘idle’ stage, the wrist typically appears lower than the elbow, 
                whereas in the ‘active’ stage, the wrist tends to be higher. This data exploration has led us to propose the 
                concept of a ‘gesture chunk’ as the unit of analysis, which we will introduce in the following section.
            </p>


            <h4 class="subtitle">'Gesture Chunk' as a New Unit of Analysis in Abstraction</h4>
            <p class="paragraph">
                In our exploratory data analysis, we introduce the concept of a ‘gesture chunk’ as the unit of 
                analysis for this dataset. Observations show that students alternate between ‘idle’ and ‘active’ 
                states during video recordings. We define a gesture chunk as a segment of body movement occurring 
                during an active state, making it a distinguishable and countable unit that comprises a continuous 
                section of frames within the original video.

                To facilitate our analysis, we propose constructing feature fields within the gesture chunk structure. 
                The first feature involves segmentation: identifying each ‘idle’ and ‘active’ state to determine the 
                bounding frames for each gesture chunk. This segmentation allows us to pinpoint all gesture chunks 
                in a video file. Additionally, we can compute segment-related features such as the starting frame 
                ID, starting timestamp, ending frame ID, ending timestamp, and duration.
                
                To simplify our feature engineering process, we separate the analysis of hand gestures from arm 
                gestures. For hand gesture detection, the second type of feature, we plan to utilize existing models 
                to identify the gestures of each hand in every frame of a gesture chunk.
                
                The final feature we focus on is keyframe extraction, akin to the reverse process of creating an 
                animation by setting keyframes. This feature involves selecting a subset of frames within a gesture 
                chunk, primarily focusing on the motion of the wrist landmarks. Although additional features may be 
                considered in future developments, in this project, our focus is strictly on establishing baseline 
                algorithms for segmentation, hand gesture detection, and keyframe extraction.
            </p>
        </div>
      </div>
    </section>

    
    <!-- Segmentation  -->
    <section id="segmentation">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Segmentation</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                Todo...Introduce methods for segmentation
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                Todo...Display results.
            </p>

            </p>
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                Todo...add comments to current algorithms and results.
            </p>
          </div>
        </div>
    </section>


    <!-- Hand Gesture Detection  -->
    <section id="handgesture">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Hand Gesture Detection</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                Todo...Introduce methods
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                Todo...Display results
            </p>

            </p>
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                Todo...add comments to current algorithms and results.
            </p>
          </div>
        </div>
    </section>


    <!-- Keyframe Extraction  -->
    <section id="keyframe">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Keyframe Extraction</h3>
            <h4 class="subtitle">Method</h4>
            <p class="paragraph">
                A <span style="font-style: italic;">gesture chunk</span> is a continuous segment of frames from the original video data, 
                capturing the animation of gestures in an active stage. The purpose of keyframe extraction is to represent this animation 
                using fewer frames. For simplicity, this baseline keyframe extraction focuses solely on the movement of the arms, ignoring 
                hand movements. Consequently, the landmarks of the wrists (left and right) are critical for selecting keyframes. 
                At a high level, we aim to disregard frames where the wrist’s position remains nearly unchanged. If the wrist moves from 
                position <span style="font-style: italic;">a</span> to <span style="font-style: italic;">b</span> along a roughly straight 
                line, we designate <span style="font-style: italic;">a</span> and <span style="font-style: italic;">b</span> as keyframes 
                and use linear interpolation to compute all intervening frames. To achieve this, we primarily use the velocity and positions 
                of the wrists’ landmarks to filter the keyframes. Initially, we identify frames where the wrist velocity is minimal (near zero). 
                Next, we examine the wrists’ positions for a second round of filtering. After these two stages of filtering on each of the wrist data, 
                we then union the two sets to extract the keyframe sequence for the given gesture chunk.
            </p>
            <h4 class="subtitle">Results</h4>
            <p class="paragraph">
                The videos below showcase a result example from our dataset, specifically selected 
                from original data (MAP014). The scenario shows a student answering the question: 
                <i>Are the lengths of the two diagonals of a rectangle the same?</i> In the first half of 
                the video, the student keep raising his arms to represent a rectangle. In the second 
                half, he swings his upper arms back and forth 8 times to depict the diagonals of the 
                rectangle. Video.1, created from the original data, contains 129 frames. It can be 
                clearly seen the student’s whole body micro-movements even though he remains in a fixed position. 

                Video.2 is constructed from 15 extracted keyframes generated by our algorithm, which selects 
                frames 0, 5, 7, 13, 70, 75, 93, 99, 100, 103, 107, 109, 121, 124, and 128 as keyframes. 
                By employing linear interpolation, we reconstruct the other frames. This video utilizes 
                normalized data, where landmarks have been transformed to the fixed shoulder coordinate 
                system, significantly reducing noise from whole body micro-movements. This keyframe 
                extraction process not only reduces the number of frames needed to represent the gesture 
                chunk but also further minimizes noise from micro-movements. For this dataset, the current 
                keyframe extraction algorithm is estimated to extract keyframes amounting to 10% to 20% of 
                the total number of original frames.
                <br><br>
                <span style="color:rgb(115, 93, 176)">
                    <b>Note</b>: If these videos are not playing in Google Chrome, please try using Safari instead.
                </span>
            </p>

    
            <div class="video-container">
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/PRE.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.1 </b></span> Made from 129 original frames.
                    </p>
                </div>
                <div class="video-box">
                    <video controls>
                        <source src="assets/video/KEY.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="v_description">
                        <span><b>Video.2 </b></span> Made from 15 extracted keyframes.
                    </p>
                </div>
            </div>
            
            
            <h4 class="subtitle">Limitations</h4>
            <p class="paragraph">
                The baseline algorithm performs adequately for arm-based gestures, as demonstrated by the results provided. However, 
                several potential limitations exist: 1) The algorithm may not effectively track tracing-based gestures, such as a 
                student using their index finger to draw a circle in the air. 2) It does not account for hand gestures, focusing only 
                on arm movements. 3) Linear interpolation may not accurately represent the arc-like movements of the wrists. 4) The issue 
                of feature normalization, particularly how to handle the time gap between each pair of keyframes, remains unexplored. Overall, 
                there are several aspects of this baseline algorithm that could be improved to enhance the robustness of keyframe feature 
                extraction and its suitability for addressing high-level questions.
            </p>
          </div>
        </div>
    </section>


    <!-- ======= Conclusion Section ======= -->
    <section id="conclusion">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h3 class="section-title">Conclusion</h3>
          <h4 class="subtitle">Summary</h4>
            <p class="paragraph">
                In our exploratory data analysis of a portion of the dataset, we focused primarily on the 
                landmarks of shoulders, elbows, and wrists. We have successfully demonstrated the feasibility 
                of transforming these landmarks into a fixed-shoulder coordinate system, which allows for 
                normalization across varying participant-screen distances. Additionally, we discovered that the 
                relative heights of the elbow and wrist serve as effective predictors for distinguishing 
                between ‘idle’ and ‘active’ stages of movement. We introduced the concept of a ‘gesture chunk’ as 
                our unit of analysis, which contains a section of continuous frames within an ‘active’ stage. 
                To support this framework, we developed a baseline algorithm that constructs the fields of the 
                gesture chunk structure: segmentation, hand gesture detection, and keyframe extraction. 
                These components collectively form the feature layer for the dataset, paving the way for 
                further analysis and model development.
            </p>
            <h4 class="subtitle">Future Directions</h4>
            <p class="paragraph">
                Looking ahead, our efforts will focus on two primary objectives. The first is to enhance 
                the robustness and applicability of the current baseline algorithm for constructing a deep 
                learning model. This includes making the segmentation algorithm more capable of incorporating 
                states of low wrist activity and refining the hand gesture detection model to improve accuracy 
                across a broader range of hand gestures. Additionally, we aim to advance keyframe extraction 
                techniques to better handle tracing-based gestures and integrate these improvements with hand 
                gesture detection. Another goal is to expand our analytical framework by including additional 
                variables that will make the gesture chunk structure more relevant for our analysis. The second 
                objective involves developing models for both unsupervised learning, such as clustering, 
                and supervised learning, which will require a more extensive labeled dataset. These models are 
                intended to aid in the identification of high-level abstract gestures, marking a significant step 
                towards achieving our ultimate goal.
            </p>
        </div>
      </div>
    </section>

    <section id="reference" style="margin-bottom: 20%;">
        <div class="container" data-aos="fade-up">
          <div class="section-header">
            <h3 class="section-title">Reference</h3>
            
            <p style="color:#333;">
                Abrahamsen, D., Nathan, M. J., Williams-Pierce, C., Walkington, C., Ottmar, E. R., Soto, H., & Alibali, M. W. (2020). 
                <i>The Future of Embodied Design for Mathematics Teaching and Learning</i>. Frontiers in Education, STEM Education. 
                Retrieved from <a href="https://doi.org/10.3389/feduc.2020.00147" target="_blank">https://doi.org/10.3389/feduc.2020.00147</a>
            </p>



          </div>
        </div>
      </section>

</main>

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>